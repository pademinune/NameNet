
r1.1 is the exact same architecture as r1, but it is trained on 40 epochs with batch size 1

Epoch 0 loss: 0.40898
Epoch 4 loss: 0.34930
Epoch 8 loss: 0.33896
Epoch 12 loss: 0.33381
Epoch 16 loss: 0.33434
Epoch 20 loss: 0.33470
Epoch 24 loss: 0.33542
Epoch 28 loss: 0.33367
Epoch 32 loss: 0.33654
Epoch 36 loss: 0.33612
TRAINING FINISHED

Notes:
- The loss seems to converge above 0
    - this means the rnn may not have enough parameters
